# Model Configuration for Object Detection using Faster R-CNN with NASNet Feature Extractor
model {
  faster_rcnn {
    num_classes: 3  # Number of classes for classification in the problem

    image_resizer {
      fixed_shape_resizer {  # Use fixed shape resizing instead of keep_aspect_ratio_resizer
        height: 640  # Height of the image after resizing
        width: 640  # Width of the image after resizing
      }
    }

    feature_extractor {
      type: "faster_rcnn_nasnet_mobile"  # Use NASNet Mobile as the feature extractor
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.9999998989515007e-05  # Weight for L2 regularization
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0  # Mean value of the initializer
            stddev: 0.029999999329447746  # Standard deviation of the initializer
          }
        }
        activation: SWISH  # Activation function used is SWISH
        batch_norm {
          decay: 0.9900000095367432  # Decay rate for batch normalization
          scale: true  # Whether to use scale in batch normalization
          epsilon: 0.0010000000474974513  # Epsilon value for batch normalization
        }
        force_use_bias: true  # Force the use of bias in convolutional layers
      }
    }

    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0  # Scale for the y dimension (updated)
        x_scale: 10.0  # Scale for the x dimension (updated)
        height_scale: 5.0  # Scale for height (updated)
        width_scale: 5.0  # Scale for width (updated)
      }
    }

    matcher {
      argmax_matcher {
        matched_threshold: 0.5  # Threshold for determining a bounding box as matched
        unmatched_threshold: 0.5  # Threshold for determining a bounding box as unmatched
        ignore_thresholds: false  # Do not ignore thresholds
        negatives_lower_than_unmatched: true  # Negative boxes have lower scores than unmatched boxes
        force_match_for_each_row: true  # Ensure each row has at least one match
        use_matmul_gather: true  # Use matrix multiplication to gather matches
      }
    }

    similarity_calculator {
      iou_similarity {  # IOU similarity for matching
      }
    }

    box_predictor {
      convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.9999998989515007e-05  # Weight for L2 regularization in box predictor
            }
          }
          initializer {
            random_normal_initializer {
              mean: 0.0  # Mean value of initializer for box predictor
              stddev: 0.009999999776482582  # Standard deviation of initializer for box predictor
            }
          }
          activation: SWISH  # Activation function is SWISH for box predictor
          batch_norm {
            decay: 0.9900000095367432  # Decay rate for batch normalization in box predictor
            scale: true  # Whether to use scale in batch normalization for box predictor
            epsilon: 0.0010000000474974513  # Epsilon value for batch normalization in box predictor
          }
          force_use_bias: true  # Force the use of bias in convolutional layers of box predictor
        }
        depth: 128  # Depth of convolutional layers in box predictor (updated)
        num_layers_before_predictor: 4  # Number of convolutional layers before the predictor (updated)
        kernel_size: 3  # Kernel size for convolutional layers in box predictor
        class_prediction_bias_init: -4.6  # Initialization bias for class prediction (updated)
        use_depthwise: true  # Use depthwise convolution in box predictor
      }
    }

    anchor_generator {
      grid_anchor_generator {
        scales: 0.25  # Scale for the smallest anchor
        scales: 0.5  # Scale for the medium anchor
        scales: 1.0  # Scale for the largest anchor
        aspect_ratios: 1.0  # Aspect ratio of anchors
        aspect_ratios: 2.0  # Aspect ratio of anchors
        aspect_ratios: 0.5  # Aspect ratio of anchors
        anchor_scale: 4.0  # Scale of anchors
      }
    }

    post_processing {
      batch_non_max_suppression {
        score_threshold: 1e-8  # Score threshold for non-max suppression (updated)
        iou_threshold: 0.6  # IOU threshold for non-max suppression (updated)
        max_detections_per_class: 100  # Maximum number of detections per class
        max_total_detections: 100  # Maximum number of detections for the whole image
      }
      score_converter: SIGMOID  # Convert scores using sigmoid
    }

    normalize_loss_by_num_matches: true  # Normalize loss by the number of matches
    loss {
      localization_loss {
        weighted_smooth_l1 {  # Smooth L1 loss for localization
        }
      }
      classification_loss {
        weighted_sigmoid_focal {
          gamma: 2.0  # Gamma parameter for focal loss (updated)
          alpha: 0.25  # Alpha parameter for focal loss
        }
      }
      classification_weight: 1.0  # Weight for classification loss
      localization_weight: 1.0  # Weight for localization loss
    }
    encode_background_as_zeros: true  # Encode background class as 0
    normalize_loc_loss_by_codesize: true  # Normalize localization loss by encoded size
    inplace_batchnorm_update: true  # Update batch normalization in place
    freeze_batchnorm: false  # Do not freeze batch normalization
    add_background_class: false  # Do not add a background class to labels
  }
}

# Training Configuration
train_config {
  batch_size: 8  # Batch size during training
  data_augmentation_options {
    random_horizontal_flip {  # Data augmentation option: random horizontal flip
    }
  }
  data_augmentation_options {
    random_crop_image {  # Data augmentation option: random crop image
      min_object_covered: 0.0  # Minimum object coverage for cropping
      min_aspect_ratio: 0.75  # Minimum aspect ratio for cropping
      max_aspect_ratio: 3.0  # Maximum aspect ratio for cropping
      min_area: 0.75  # Minimum area for cropping
      max_area: 1.0  # Maximum area for cropping
      overlap_thresh: 0.0  # Overlap threshold for cropping
    }
  }
  sync_replicas: true  # Synchronize replicas in distributed training
  optimizer {
    momentum_optimizer {
      learning_rate {
        cosine_decay_learning_rate {
          learning_rate_base: 0.08  # Initial learning rate (updated)
          total_steps: 300000  # Total number of steps for training
          warmup_learning_rate: 0.026666  # Learning rate during warmup (updated)
          warmup_steps: 1000  # Number of warmup steps (updated)
        }
      }
      momentum_optimizer_value: 0.9  # Momentum value for optimizer (updated)
    }
    use_moving_average: false  # Do not use moving average in optimizer
  }
  fine_tune_checkpoint: "Model_MobileNet/ckpt-0"  # Checkpoint for fine-tuning
  num_steps: 300000  # Total number of steps for training (updated)
  startup_delay_steps: 0.0  # Startup delay steps for training
  replicas_to_aggregate: 8  # Number of replicas to aggregate
  max_number_of_boxes: 100  # Maximum number of boxes per image
  unpad_groundtruth_tensors: false  # Do not unpad ground truth tensors
  fine_tune_checkpoint_type: "detection"  # Type of checkpoint for fine-tuning
  use_bfloat16: true  # Use bfloat16 format during training
  fine_tune_checkpoint_version: V2  # Checkpoint version for fine-tuning
}

# Training Data Input Configuration
train_input_reader: {
  label_map_path: "/opt/ml/input/data/train/label_map.pbtxt"  # Path to the label map file
  tf_record_input_reader {
    input_path: "/opt/ml/input/data/train/*.tfrecord"  # Path to TFRecord files for training
  }
}

# Evaluation Configuration
eval_config: {
  metrics_set: "coco_detection_metrics"  # Metrics set used for evaluation
  use_moving_averages: false  # Do not use moving averages in evaluation
  batch_size: 1;  # Batch size during evaluation
}

# Evaluation Data Input Configuration
eval_input_reader: {
  label_map_path: "/opt/ml/input/data/val/label_map.pbtxt"  # Path to the label map file for evaluation
  shuffle: false  # Do not shuffle data during evaluation
  num_epochs: 1  # Number of epochs for evaluation
  tf_record_input_reader {
    input_path: "/opt/ml/input/data/val/*.tfrecord"  # Path to TFRecord files for evaluation
  }
}
