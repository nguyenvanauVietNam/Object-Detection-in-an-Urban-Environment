# Model Configuration for Object Detection using Faster R-CNN with NASNet Feature Extractor
model {
  faster_rcnn {
    num_classes: 3  # Number of classes for classification in the problem
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 640  # Minimum dimension of image after resizing
        max_dimension: 640  # Maximum dimension of image after resizing
        pad_to_max_dimension: true  # Pad to achieve max_dimension
      }
    }
    feature_extractor {
      type: "faster_rcnn_nasnet_mobile"  # Use NASNet Mobile as the feature extractor
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.9999998989515007e-05  # Weight for L2 regularization
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0  # Mean value of the initializer
            stddev: 0.029999999329447746  # Standard deviation of the initializer
          }
        }
        activation: SWISH  # Activation function used is SWISH
        batch_norm {
          decay: 0.9900000095367432  # Decay rate for batch normalization
          scale: true  # Whether to use scale in batch normalization
          epsilon: 0.0010000000474974513  # Epsilon value for batch normalization
        }
        force_use_bias: true  # Force the use of bias in convolutional layers
      }
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 1.0  # Scale for the y dimension
        x_scale: 1.0  # Scale for the x dimension
        height_scale: 1.0  # Scale for height
        width_scale: 1.0  # Scale for width
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5  # Threshold for determining a bounding box as matched
        unmatched_threshold: 0.5  # Threshold for determining a bounding box as unmatched
        ignore_thresholds: false  # Do not ignore thresholds
        negatives_lower_than_unmatched: true  # Negative boxes have lower scores than unmatched boxes
        force_match_for_each_row: true  # Ensure each row has at least one match
        use_matmul_gather: true  # Use matrix multiplication to gather matches
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.9999998989515007e-05  # Weight for L2 regularization in box predictor
            }
          }
          initializer {
            random_normal_initializer {
              mean: 0.0  # Mean value of initializer for box predictor
              stddev: 0.009999999776482582  # Standard deviation of initializer for box predictor
            }
          }
          activation: SWISH  # Activation function is SWISH for box predictor
          batch_norm {
            decay: 0.9900000095367432  # Decay rate for batch normalization in box predictor
            scale: true  # Whether to use scale in batch normalization for box predictor
            epsilon: 0.0010000000474974513  # Epsilon value for batch normalization in box predictor
          }
          force_use_bias: true  # Force the use of bias in convolutional layers of box predictor
        }
        depth: 88  # Depth of convolutional layers in box predictor
        num_layers_before_predictor: 3  # Number of convolutional layers before the predictor
        kernel_size: 3  # Kernel size for convolutional layers in box predictor
        class_prediction_bias_init: -4.599999904632568  # Initialization bias for class prediction
        use_depthwise: true  # Use depthwise convolution in box predictor
      }
    }
    anchor_generator {
      grid_anchor_generator {
        scales: 0.25  # Scale for the smallest anchor
        scales: 0.5  # Scale for the medium anchor
        scales: 1.0  # Scale for the largest anchor
        aspect_ratios: 1.0  # Aspect ratio of anchors
        aspect_ratios: 2.0  # Aspect ratio of anchors
        aspect_ratios: 0.5  # Aspect ratio of anchors
        anchor_scale: 4.0  # Scale of anchors
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 0.01  # Score threshold for non-max suppression
        iou_threshold: 0.5  # IOU threshold for non-max suppression
        max_detections_per_class: 100  # Maximum number of detections per class
        max_total_detections: 100  # Maximum number of detections for the whole image
      }
      score_converter: SIGMOID  # Convert scores using sigmoid
    }
    normalize_loss_by_num_matches: true  # Normalize loss by the number of matches
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid_focal {
          gamma: 1.5  # Gamma parameter for focal loss
          alpha: 0.25  # Alpha parameter for focal loss
        }
      }
      classification_weight: 1.0  # Weight for classification loss
      localization_weight: 1.0  # Weight for localization loss
    }
    encode_background_as_zeros: true  # Encode background class as 0
    normalize_loc_loss_by_codesize: true  # Normalize localization loss by encoded size
    inplace_batchnorm_update: true  # Update batch normalization in place
    freeze_batchnorm: false  # Do not freeze batch normalization
    add_background_class: false  # Do not add a background class to labels
  }
}

# Training Configuration
train_config {
  batch_size: 8  # Batch size during training
  data_augmentation_options {
    random_horizontal_flip {  # Data augmentation option: random horizontal flip
    }
  }
  data_augmentation_options {
    random_scale_crop_and_pad_to_square {
      output_size: 640  # Output size after crop and pad
      scale_min: 0.10000000149011612  # Minimum scale for crop
      scale_max: 2.0  # Maximum scale for crop
    }
  }
  sync_replicas: true  # Synchronize replicas in distributed training
  optimizer {
    momentum_optimizer {
      learning_rate {
        cosine_decay_learning_rate {
          learning_rate_base: 0.07999999821186066  # Initial learning rate
          total_steps: 300000  # Total number of steps for training
          warmup_learning_rate: 0.0010000000474974513  # Learning rate during warmup
          warmup_steps: 2500  # Number of warmup steps
        }
      }
      momentum_optimizer_value: 0.8999999761581421  # Momentum value for optimizer
    }
    use_moving_average: false  # Do not use moving average in optimizer
  }
  fine_tune_checkpoint: "checkpoint/ckpt-0"  # Checkpoint for fine-tuning
  num_steps: 300000  # Total number of steps for training
  startup_delay_steps: 0.0  # Startup delay steps for training
  replicas_to_aggregate: 8  # Number of replicas to aggregate
  max_number_of_boxes: 100  # Maximum number of boxes per image
  unpad_groundtruth_tensors: false  # Do not unpad ground truth tensors
  fine_tune_checkpoint_type: "detection"  # Type of checkpoint for fine-tuning
  use_bfloat16: true  # Use bfloat16 format during training
  fine_tune_checkpoint_version: V2  # Checkpoint version for fine-tuning
}

# Training Data Input Configuration
train_input_reader: {
  label_map_path: "/opt/ml/input/data/train/label_map.pbtxt"  # Path to the label map file
  tf_record_input_reader {
    input_path: "/opt/ml/input/data/train/*.tfrecord"  # Path to TFRecord files for training
  }
}

# Evaluation Configuration
eval_config: {
  metrics_set: "coco_detection_metrics"  # Metrics set used for evaluation
  use_moving_averages: false  # Do not use moving averages in evaluation
  batch_size: 1;  # Batch size during evaluation
}

# Evaluation Data Input Configuration
eval_input_reader: {
  label_map_path: "/opt/ml/input/data/val/label_map.pbtxt"  # Path to the label map file for evaluation
  shuffle: false  # Do not shuffle data during evaluation
  num_epochs: 1  # Number of epochs for evaluation
  tf_record_input_reader {
    input_path: "/opt/ml/input/data/val/*.tfrecord"  # Path to TFRecord files for evaluation
  }
}
